setwd("C:/Users/anany/Dropbox/Data Analytics/F25-DataAnalytics/Lab4")
## load libraries
library(ggplot2)
library(ggfortify)
library(GGally)
library(e1071)
library(class)
library(psych)
library(readr)
## read dataset
wine <- read_csv("wine.data", col_names = FALSE)
## set column names
names(wine) <- c("Type","Alcohol","Malic acid","Ash","Alcalinity of ash","Magnesium","Total phenols","Flavanoids","Nonflavanoid Phenols","Proanthocyanins","Color Intensity","Hue","Od280/od315 of diluted wines","Proline")
## inspect data frame
head(wine)
View(wine)
wine$Type <- as.factor(wine$Type)
## visualize variables
pairs.panels(wine[,-1],gap = 0,bg = c("red", "yellow", "blue")[wine$Type],pch=21)
ggpairs(wine, ggplot2::aes(colour = Type))
wine$Type <- as.factor(wine$Type)
## visualize variables
pairs.panels(wine[,-1],gap = 0,bg = c("red", "yellow", "blue")[wine$Type],pch=21)
ggpairs(wine, ggplot2::aes(colour = Type))
View(wine)
X <- wine[,2:6]
Y <- wine[,7:13]
head(X)
Y <- wine[,7:13]
head(Y)
X <- wine[,2:7]
Y <- wine[,8:13]
head(X)
head(Y)
## visualize variables
pairs.panels(X[,-1],gap = 0,bg = c("red", "yellow", "blue")[wine$Type],pch=21)
ggpairs(X, ggplot2::aes(colour = Type))
X <- wine[1,2:7]
Y <- wine[1,8:13]
## visualize variables
pairs.panels(X[,-1],gap = 0,bg = c("red", "yellow", "blue")[wine$Type],pch=21)
X <- wine[, c(1, 2:7)]
Y <- wine[, c(1, 8:13)]
## visualize variables
pairs.panels(X[,-1],gap = 0,bg = c("red", "yellow", "blue")[wine$Type],pch=21)
ggpairs(X, ggplot2::aes(colour = Type))
pairs.panels(Y[,-1],gap = 0,bg = c("orange", "green", "purple")[wine$Type],pch=21)
X <- wine[, 2:7]
Y <- wine[,8:13]
ggpairs(data.frame(Type, Y), ggplot2::aes(colour = Type))
Type <- wine$Type
ggpairs(data.frame(Type, Y), ggplot2::aes(colour = Type))
Xc <- scale(Xmat, center = T, scale = F)
Xmat <- as.matrix(X)
Xc <- scale(Xmat, center = T, scale = F)
principal_components <- princomp(Xc)
summary(principal_components
summary(principal_components)
Xmat <- as.matrix(X)
Xc <- scale(Xmat, center = T, scale = F)
principal_components <- princomp(Xc)
summary(principal_components)
principal_components$loadings
biplot(principal_components)
autoplot(principal_components, data = iris, colour = 'Species',
loadings = TRUE, loadings.colour = 'blue',
loadings.label = TRUE, loadings.label.size = 3, scale = 0)
autoplot(principal_components, data = X, colour = 'Type',
loadings = TRUE, loadings.colour = 'blue',
loadings.label = TRUE, loadings.label.size = 3, scale = 0)
pca_all <- prcomp(wine[, 2:14], center = TRUE, scale. = TRUE)
summary(pca_all)           # variance explained
pca_all$rotation
pca_all$loadings
pca_all$x
# Biplot with types colored, loadings shown
autoplot(
pca_all,
data = wine,
colour = 'Type',
loadings = TRUE,
loadings.colour = 'blue',
loadings.label = TRUE,
loadings.label.size = 3
)
#KNN
set.seed(123)
s.train <- sample(1:nrow(wine), 0.7 * nrow(wine)) #asked chatgpt for a better sample size
wine.train <-wine[s.train,]
wine.test <-wine[-s.train,]
## kNN Model
train.features <- scale(wine.train[, -1])  #excluding type
test.features  <- scale(wine.test[, -1],
center = attr(train.features, "scaled:center"),
scale = attr(train.features, "scaled:scale"))
train.labels <- wine.train$Type
test.labels  <- wine.test$Type
## Train kNN model (k = 3 as example)
knn.predicted <- knn(train = train.features,
test = test.features,
cl = train.labels,
k = 3)
## Confusion matrix
conf_matrix <- table(Predicted = knn.predicted, Actual = test.labels)
print(conf_matrix)
## Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Accuracy:", round(accuracy * 100, 2), "%\n")
# creat training and testing sets
iris.train <-iris[s.train,]
iris.test <-iris[-s.train,]
## kNN Model
knn.predicted <- knn(iris.train[,1:4], iris.test[,1:4], iris.train[,5], k=3)
dim(iris.test)
dim(iris.train)
## scatter plot of 2 variables for training set
ggplot(iris.train, aes(x = Petal.Length, y = Petal.Width, colour = Species)) +
geom_point()
## scatter plot of 2 variables for test set
ggplot(iris.test, aes(x = Petal.Length, y = Petal.Width, colour = Species)) +
geom_point()
# Decision tree model
tree.model <- rpart(Species~., iris.train, method = "class")
tree.model
#plotting the decision tree model using rpart.plot() function
rpart.plot(tree.model)
tree.model.predicted <-  predict(tree.model, iris.test, type = "class")
## confusion matrix/contingency table
table(tree.model.predicted, iris.test$Species, dnn=list('predicted','actual'))
tree.model.predicted <-  predict(tree.model, iris.test, type = "class")
# Install the following libararies/packages in RStudio
library(ggplot2)
library(rpart)
library(rpart.plot)
library(class)
library(randomForest)
# we will be using the iris dataset
iris.data <- iris
dim(iris.data)
## scatter plot of 2 variables
ggplot(iris.data, aes(x = Sepal.Length, y = Petal.Width, colour = Species)) +
geom_point()
## scatter plot of 2 variables
ggplot(iris.data, aes(x = Petal.Length, y = Petal.Width, colour = Species)) +
geom_point()
# creating a sample from the iris dataset
s.train <- sample(150,100)
s.train
# creat training and testing sets
iris.train <-iris[s.train,]
iris.test <-iris[-s.train,]
dim(iris.test)
dim(iris.train)
## scatter plot of 2 variables for training set
ggplot(iris.train, aes(x = Petal.Length, y = Petal.Width, colour = Species)) +
geom_point()
## scatter plot of 2 variables for test set
ggplot(iris.test, aes(x = Petal.Length, y = Petal.Width, colour = Species)) +
geom_point()
# Decision tree model
tree.model <- rpart(Species~., iris.train, method = "class")
tree.model
#plotting the decision tree model using rpart.plot() function
rpart.plot(tree.model)
tree.model.predicted <-  predict(tree.model, iris.test, type = "class")
## confusion matrix/contingency table
table(tree.model.predicted, iris.test$Species, dnn=list('predicted','actual'))
## kNN Model
knn.predicted <- knn(iris.train[,1:4], iris.test[,1:4], iris.train[,5], k=3)
## confusion matrix/contingency table
table(knn.predicted, iris.test[,5], dnn=list('predicted','actual'))
## Train kNN model (k = 3 as example)
knn.predicted <- knn(train = train.features,
test = test.features,
cl = train.labels,
k = 3)
## Confusion matrix
conf_matrix <- table(Predicted = knn.predicted, Actual = test.labels)
print(conf_matrix)
## Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Accuracy:", round(accuracy * 100, 2), "%\n")
#DECISION TREE
scores <- data.frame(pca_all$x[, 1:2], Type = wine$Type)
pc.train <- scores[s.train, ]
pc.test  <- scores[-s.train, ]
tree_model <- rpart(Type ~ PC1 + PC2, data = pc.train, method = "class")
tree_pred  <- predict(tree_model, newdata = pc.test, type = "class")
tree_cm <- confusionMatrix(tree_pred, pc.test$Type)
library(rpart)
library(caret)
tree_cm <- confusionMatrix(tree_pred, pc.test$Type)
cat("\n=== Decision Tree on First 2 PCs ===\n")
print(tree_cm$table)
print(tree_cm$overall['Accuracy'])
print(tree_cm$byClass[, c("Precision","Recall","F1")])
comparison <- data.frame(
Model = c("kNN (All Variables)", "Decision Tree (PC1 & PC2)"),
Accuracy = c(knn_cm$overall["Accuracy"], tree_cm$overall["Accuracy"]),
Precision = c(mean(knn_cm$byClass[, "Precision"], na.rm = TRUE),
mean(tree_cm$byClass[, "Precision"], na.rm = TRUE)),
Recall = c(mean(knn_cm$byClass[, "Recall"], na.rm = TRUE),
mean(tree_cm$byClass[, "Recall"], na.rm = TRUE)),
F1 = c(mean(knn_cm$byClass[, "F1"], na.rm = TRUE),
mean(tree_cm$byClass[, "F1"], na.rm = TRUE))
)
## Confusion matrix
knn_cm <- confusionMatrix(knn.predicted, test.labels)
print(knn_cm)
comparison <- data.frame(
Model = c("kNN (All Variables)", "Decision Tree (PC1 & PC2)"),
Accuracy = c(knn_cm$overall["Accuracy"], tree_cm$overall["Accuracy"]),
Precision = c(mean(knn_cm$byClass[, "Precision"], na.rm = TRUE),
mean(tree_cm$byClass[, "Precision"], na.rm = TRUE)),
Recall = c(mean(knn_cm$byClass[, "Recall"], na.rm = TRUE),
mean(tree_cm$byClass[, "Recall"], na.rm = TRUE)),
F1 = c(mean(knn_cm$byClass[, "F1"], na.rm = TRUE),
mean(tree_cm$byClass[, "F1"], na.rm = TRUE))
)
print(comparison)
